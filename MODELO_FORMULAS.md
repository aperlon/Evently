# üìê F√≥rmulas del Modelo de Regresi√≥n - Economic Impact Model

## üéØ Objetivo del Modelo

Predecir el **impacto econ√≥mico total** (`total_economic_impact_usd`) de un evento bas√°ndose en sus caracter√≠sticas y las de la ciudad donde se realiza.

---

## üìä Variables de Entrada (Features)

### 1. Caracter√≠sticas del Evento

- **`attendance`**: Asistencia esperada al evento
- **`duration_days`**: Duraci√≥n del evento en d√≠as
- **`event_type_encoded`**: Tipo de evento codificado (sports, music, culture, festival, business)
- **`visitor_increase_pct`**: Porcentaje de aumento de visitantes
- **`price_increase_pct`**: Porcentaje de aumento de precios hoteleros
- **`occupancy_boost`**: Aumento en ocupaci√≥n hotelera (%)

### 2. Caracter√≠sticas de la Ciudad

- **`population`**: Poblaci√≥n de la ciudad
- **`annual_tourists`**: Turistas anuales
- **`hotel_rooms`**: N√∫mero de habitaciones hoteleras
- **`avg_hotel_price_usd`**: Precio promedio de hotel (USD)

### 3. Features Derivadas (Calculadas)

#### F√≥rmula 1: Asistencia por D√≠a
```
attendance_per_day = attendance / duration_days
```

#### F√≥rmula 2: Visitantes por Habitaci√≥n
```
visitors_per_hotel_room = attendance / hotel_rooms
```

#### F√≥rmula 3: Intensidad Tur√≠stica de la Ciudad
```
city_tourism_intensity = annual_tourists / population
```

#### F√≥rmula 4: Sensibilidad de Precio (no usada en modelo final)
```
price_sensitivity = price_increase_pct / visitor_increase_pct
```

---

## üîÑ Transformaci√≥n de Datos

### Transformaci√≥n Logar√≠tmica del Target

El modelo usa **transformaci√≥n logar√≠tmica** para mejorar la distribuci√≥n de los datos:

```
y_log = log(1 + total_economic_impact_usd)
```

**Raz√≥n**: Los impactos econ√≥micos tienen una distribuci√≥n muy sesgada (algunos eventos generan millones, otros miles). La transformaci√≥n logar√≠tmica normaliza la distribuci√≥n.

### Normalizaci√≥n de Features

Todas las features se normalizan usando **StandardScaler**:

```
X_scaled = (X - Œº) / œÉ
```

Donde:
- `Œº` = media de cada feature
- `œÉ` = desviaci√≥n est√°ndar de cada feature

**Raz√≥n**: Las features tienen escalas muy diferentes (poblaci√≥n en millones, precios en cientos, etc.). La normalizaci√≥n asegura que todas tengan el mismo peso.

---

## ü§ñ Algoritmos de Regresi√≥n Utilizados

### 1. Linear Regression (Regresi√≥n Lineal)

**F√≥rmula**:
```
y = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ + ... + Œ≤‚Çôx‚Çô + Œµ
```

Donde:
- `y` = impacto econ√≥mico (en log space)
- `Œ≤‚ÇÄ` = intercepto
- `Œ≤‚ÇÅ, Œ≤‚ÇÇ, ..., Œ≤‚Çô` = coeficientes
- `x‚ÇÅ, x‚ÇÇ, ..., x‚Çô` = features
- `Œµ` = error

**Optimizaci√≥n**: M√≠nimos Cuadrados Ordinarios (OLS)
```
min Œ£(y·µ¢ - ≈∑·µ¢)¬≤
```

### 2. Ridge Regression (Regresi√≥n con Regularizaci√≥n L2)

**F√≥rmula**:
```
y = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + ... + Œ≤‚Çôx‚Çô + ŒªŒ£Œ≤·µ¢¬≤
```

Donde `Œª` (alpha=1.0) es el par√°metro de regularizaci√≥n que penaliza coeficientes grandes.

**Optimizaci√≥n**:
```
min [Œ£(y·µ¢ - ≈∑·µ¢)¬≤ + ŒªŒ£Œ≤·µ¢¬≤]
```

### 3. Lasso Regression (Regresi√≥n con Regularizaci√≥n L1)

**F√≥rmula**:
```
y = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + ... + Œ≤‚Çôx‚Çô + ŒªŒ£|Œ≤·µ¢|
```

Donde `Œª` (alpha=0.1) penaliza la suma absoluta de coeficientes, promoviendo sparsity.

**Optimizaci√≥n**:
```
min [Œ£(y·µ¢ - ≈∑·µ¢)¬≤ + ŒªŒ£|Œ≤·µ¢|]
```

### 4. Random Forest Regressor

**F√≥rmula**:
```
≈∑ = (1/B) Œ£·µ¢‚Çå‚ÇÅ·¥Æ T·µ¢(x)
```

Donde:
- `B` = n√∫mero de √°rboles (100)
- `T·µ¢(x)` = predicci√≥n del √°rbol i-√©simo

**Criterio de divisi√≥n**: Varianza reducida
```
Var(Y) - (n_left/n)Var(Y_left) - (n_right/n)Var(Y_right)
```

### 5. Gradient Boosting Regressor

**F√≥rmula** (aditivo):
```
F‚Çò(x) = F‚Çò‚Çã‚ÇÅ(x) + Œ±‚Çòh‚Çò(x)
```

Donde:
- `F‚Çò(x)` = modelo despu√©s de m iteraciones
- `Œ±‚Çò` = learning rate (0.1)
- `h‚Çò(x)` = √°rbol d√©bil en iteraci√≥n m

**P√©rdida**: Error cuadrado
```
L(y, F(x)) = (y - F(x))¬≤
```

**Gradiente**:
```
-‚àÇL/‚àÇF = 2(y - F(x))
```

---

## üìà M√©tricas de Evaluaci√≥n

### 1. R¬≤ Score (Coeficiente de Determinaci√≥n)

```
R¬≤ = 1 - (SS_res / SS_tot)
```

Donde:
- `SS_res = Œ£(y·µ¢ - ≈∑·µ¢)¬≤` (Suma de cuadrados residual)
- `SS_tot = Œ£(y·µ¢ - »≥)¬≤` (Suma de cuadrados total)
- `»≥` = media de y

**Interpretaci√≥n**: 
- R¬≤ = 1.0 ‚Üí Predicci√≥n perfecta
- R¬≤ = 0.0 ‚Üí Modelo no mejor que predecir la media
- R¬≤ < 0.0 ‚Üí Modelo peor que predecir la media

### 2. MAE (Mean Absolute Error)

```
MAE = (1/n) Œ£|y·µ¢ - ≈∑·µ¢|
```

**Interpretaci√≥n**: Error promedio en d√≥lares.

### 3. RMSE (Root Mean Squared Error)

```
RMSE = ‚àö[(1/n) Œ£(y·µ¢ - ≈∑·µ¢)¬≤]
```

**Interpretaci√≥n**: Error promedio (penaliza m√°s los errores grandes).

### 4. MAPE (Mean Absolute Percentage Error)

```
MAPE = (100/n) Œ£|(y·µ¢ - ≈∑·µ¢) / y·µ¢|
```

**Interpretaci√≥n**: Error porcentual promedio.

### 5. Cross-Validation R¬≤ (5-fold)

```
CV_R¬≤ = (1/k) Œ£·µ¢‚Çå‚ÇÅ·µè R¬≤·µ¢
```

Donde `k=5` folds.

---

## üîÆ Predicci√≥n

### Paso 1: Preparar Features

```python
features = [
    attendance,
    duration_days,
    event_type_encoded,
    visitor_increase_pct,
    price_increase_pct,
    occupancy_boost,
    population,
    annual_tourists,
    hotel_rooms,
    avg_hotel_price_usd,
    attendance / duration_days,           # attendance_per_day
    attendance / hotel_rooms,              # visitors_per_hotel_room
    annual_tourists / population           # city_tourism_intensity
]
```

### Paso 2: Normalizar

```
X_scaled = scaler.transform(features)
```

### Paso 3: Predecir (en log space)

```
y_pred_log = best_model.predict(X_scaled)
```

### Paso 4: Transformar de vuelta

```
prediction = exp(y_pred_log) - 1
```

Usando `expm1` para mayor precisi√≥n num√©rica:
```
prediction = expm1(y_pred_log)
```

### Paso 5: Calcular Intervalo de Confianza

```
lower_bound = prediction √ó (1 - MAPE √ó 1.5)
upper_bound = prediction √ó (1 + MAPE √ó 1.5)
```

Donde `MAPE` es el MAPE del modelo en entrenamiento.

---

## üí∞ Desglose del Impacto Econ√≥mico

### F√≥rmulas de Desglose

```
direct_spending = total_impact √ó 0.64      # 64% gasto directo
indirect_spending = total_impact √ó 0.25    # 25% gasto indirecto
induced_spending = total_impact √ó 0.11     # 11% gasto inducido
```

### Estimaci√≥n de Empleos

```
jobs_created = total_impact / 40,000
```

**Raz√≥n**: Estimaci√≥n de $40,000 por empleo creado.

### Estimaci√≥n de ROI

```
estimated_cost = total_impact / 4.0
roi_ratio = total_impact / estimated_cost
```

**Raz√≥n**: ROI t√≠pico de 4:1 (por cada $1 invertido, se generan $4).

---

## üéØ Selecci√≥n del Mejor Modelo

El modelo se selecciona bas√°ndose en el **mayor R¬≤ Score**:

```python
best_model = argmax(R¬≤_score)
```

Si el mejor modelo es tree-based (Random Forest o Gradient Boosting), se calcula la **importancia de features**:

```
importance_i = (1/n_trees) Œ£·µ¢‚Çå‚ÇÅ‚Åø·µó ≥·µâ·µâÀ¢ importance_i_tree_j
```

---

## üìù Notas Importantes

1. **Transformaci√≥n Logar√≠tmica**: El modelo predice en log space para manejar mejor la escala de los datos.

2. **Normalizaci√≥n**: Todas las features se normalizan para que tengan el mismo peso.

3. **Cross-Validation**: Se usa 5-fold CV para evaluar robustez del modelo.

4. **Regularizaci√≥n**: Ridge y Lasso usan regularizaci√≥n para evitar overfitting.

5. **Ensemble Methods**: Random Forest y Gradient Boosting combinan m√∫ltiples √°rboles para mejor predicci√≥n.

